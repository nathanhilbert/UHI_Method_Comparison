{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Urban Definition\n",
    "--------------\n",
    "Them:\n",
    "    - Use the closest station that has data from 1970-2013 to the urban center and select that one urban\n",
    "    - For the Rural selection:\n",
    "       - Between 50km - 250km away\n",
    "       - Be in a population of < 10k\n",
    "       - lie in a dim, dark, or unlight nighttime lights area\n",
    "Us:\n",
    "    - Use the closest station that has data from 2004-2013 to the urban center and select that one urban based\n",
    "    - Add 250km buffer around the selected station and select the closest station that meets the following criterea:\n",
    "        - Must be greater than 50km\n",
    "        - Landscan population of less than a value of 193 as proxy for the census data and nighttime lights\n",
    "Suggest:\n",
    "    - Using elevation\n",
    "\n",
    "Timeframe and data \n",
    "----------------\n",
    "2004-2013 \n",
    "Prism, Dayment, Station Data, Modis\n",
    "\n",
    "Function\n",
    "----------------\n",
    "(Urban & tMin) - (Rural & tMin)\n",
    "\n",
    "(Urban & tAvg) - (Rural & tAvg)\n",
    "\n",
    "\n",
    "Stats\n",
    "-----------------\n",
    " - June, July, August 2004-2013\n",
    " - Average summer (June to Aug daily temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MULTIPOINT (-97.33754479999999 37.69223609908695)\n"
     ]
    }
   ],
   "source": [
    "# Find the closest station to downtown\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from shapely import wkb\n",
    "import requests\n",
    "POSTGRESURI = 'postgresql://urbis:urbis@ontoserv:5434/urbisdata01'\n",
    "engine = create_engine(POSTGRESURI)\n",
    "\n",
    "SELECTPLACES = \"\"\"\n",
    "SELECT \n",
    "(array_agg(earthenv.placeid ORDER BY usgscities.\"pop_2010\" DESC))[1] AS placeid,\n",
    "(array_agg(usgscities.name ORDER BY usgscities.\"pop_2010\" DESC))[1] AS usgsplacename,\n",
    "(array_agg(ST_AsEWKB(ST_Transform(usgscities.geom, 4326)) ORDER BY usgscities.\"pop_2010\" DESC))[1] AS usgsplacegeomwgs84str,\n",
    "(array_agg(ST_AsEWKB(usgscities.geom) ORDER BY usgscities.\"pop_2010\" DESC))[1] AS usgsplacegeomstr,\n",
    "(array_agg(usgscities.\"pop_2010\"  ORDER BY usgscities.\"pop_2010\" DESC))[1] AS usgspopulation,\n",
    "(array_agg(usgscities.countyfips  ORDER BY usgscities.\"pop_2010\" DESC))[1] AS countryfips,\n",
    "(array_agg(usgscities.\"state_fips\"  ORDER BY usgscities.\"pop_2010\" DESC))[1] AS statefips\n",
    "FROM urbanclusters.usgscities as usgscities, \n",
    "urbanclusters.earthenv_urbannamed as earthenv\n",
    "WHERE ST_Intersects(usgscities.geom, earthenv.geom) \n",
    "GROUP BY earthenv.placeid\n",
    "ORDER BY usgspopulation DESC\n",
    "LIMIT 100 \"\"\"\n",
    "\n",
    "placeresult = engine.execute(SELECTPLACES)\n",
    "\n",
    "sampleplaces = {}\n",
    "\n",
    "for row in placeresult:\n",
    "    rowdict = dict(row)\n",
    "    rowdict['usgsplacegeom'] = wkb.loads(str(rowdict[\"usgsplacegeomstr\"]))\n",
    "    rowdict['usgsplacegeomwgs84'] = wkb.loads(str(rowdict[\"usgsplacegeomwgs84str\"]))\n",
    "    sampleplaces[rowdict['placeid']] = rowdict\n",
    "    \n",
    "\n",
    "\n",
    "earthenvtable = 'urbanclusters.earthenv_urbannamed'\n",
    "\n",
    "newsamples = {}\n",
    "\n",
    "for placeid in sampleplaces.keys():\n",
    "\n",
    "    GETGEOM = \"\"\"\n",
    "        SELECT ST_AsEWKB(geom), ST_AsEWKB(ST_Transform(geom, 4326)) as wgs84geom,\n",
    "        ST_AsEWKB(ST_Transform(ST_Difference(\n",
    "        ST_Buffer(geom, sqrt(St_Area(geom)/pi())*2)\n",
    "        , geom), 4326)) AS ruralgeomwgs84,\n",
    "        ST_AsEWKB(ST_Difference(\n",
    "        ST_Buffer(geom, sqrt(St_Area(geom)/pi())*2)\n",
    "        , geom)) AS ruralgeom\n",
    "        FROM {0}\n",
    "        WHERE placeid={1}\n",
    "        \"\"\".format(earthenvtable, placeid)\n",
    "    r = engine.execute(GETGEOM)\n",
    "    firstitem = r.first()\n",
    "    if firstitem:\n",
    "        newsamples[placeid] = sampleplaces[placeid]\n",
    "        \n",
    "        newsamples[placeid][\"earthenv\"] = {\n",
    "            'geom': wkb.loads(str(firstitem[0])),\n",
    "            'wgs84': wkb.loads(str(firstitem[1])),\n",
    "            'ruralgeom': wkb.loads(str(firstitem[3])),\n",
    "            'ruralgeomwgs84': wkb.loads(str(firstitem[2])),\n",
    "        }\n",
    "sampleplaces = newsamples\n",
    "print sampleplaces.values()[0]['usgsplacegeomwgs84']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get urban and rural stations\n",
    "from shapely.geometry import Point\n",
    "import acis\n",
    "import json\n",
    "\n",
    "for s in sampleplaces.values():\n",
    "\n",
    "    bbox = \",\".join([str(x) for x in s['earthenv']['ruralgeomwgs84'].bounds])\n",
    "\n",
    "    closeststation = None\n",
    "    res = requests.get(\"http://data.rcc-acis.org/StnMeta?bbox={0}&sdate=2004-01-01&edate=2013-12-31&output=json\".format(bbox))\n",
    "    stationresults = json.loads(res.text)\n",
    "    urbanstations = []\n",
    "    ruralstations = []\n",
    "    for station in stationresults['meta']:\n",
    "        if len(station['sids']) == 0:\n",
    "            continue\n",
    "        stationpoint = Point(station['ll'])\n",
    "        if stationpoint.within(s['earthenv']['wgs84']):\n",
    "            urbanstations.append(station)\n",
    "        elif stationpoint.within(s['earthenv']['ruralgeomwgs84']):\n",
    "            ruralstations.append(station)\n",
    "    s['allstations'] = {\n",
    "        'urbanstations': urbanstations,\n",
    "        'ruralstations': ruralstations\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing 1 out of  100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:32: FutureWarning: convert_objects is deprecated.  Use the data-type specific converters pd.to_datetime, pd.to_timedelta and pd.to_numeric.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing 2 out of  100\n",
      "Doing 3 out of  100\n",
      "Doing 4 out of  100\n",
      "Doing 5 out of  100\n",
      "Doing 6 out of  100\n",
      "Doing 7 out of  100\n",
      "Doing 8 out of  100\n",
      "Doing 9 out of  100\n",
      "Doing 10 out of  100\n",
      "Doing 11 out of  100\n",
      "Doing 12 out of  100\n",
      "Doing 13 out of  100\n",
      "Doing 14 out of  100\n",
      "Doing 15 out of  100\n",
      "Doing 16 out of  100\n",
      "Doing 17 out of  100\n",
      "Doing 18 out of  100\n",
      "Doing 19 out of  100\n",
      "Doing 20 out of  100\n",
      "Doing 21 out of  100\n",
      "Doing 22 out of  100\n",
      "Doing 23 out of  100\n",
      "Doing 24 out of  100\n",
      "Doing 25 out of  100\n",
      "Doing 26 out of  100\n",
      "Doing 27 out of  100\n",
      "Doing 28 out of  100\n",
      "Doing 29 out of  100\n",
      "Doing 30 out of  100\n",
      "Doing 31 out of  100\n",
      "Doing 32 out of  100\n",
      "Doing 33 out of  100\n",
      "Doing 34 out of  100\n",
      "Doing 35 out of  100\n",
      "Doing 36 out of  100\n",
      "Doing 37 out of  100\n",
      "Doing 38 out of  100\n",
      "Doing 39 out of  100\n",
      "Doing 40 out of  100\n",
      "Doing 41 out of  100\n",
      "Doing 42 out of  100\n",
      "Doing 43 out of  100\n",
      "Doing 44 out of  100\n",
      "HTTP Error 503: Service Unavailable\n",
      "Doing 45 out of  100\n",
      "Doing 46 out of  100\n",
      "Doing 47 out of  100\n",
      "Doing 48 out of  100\n",
      "Doing 49 out of  100\n",
      "Doing 50 out of  100\n",
      "Doing 51 out of  100\n",
      "Doing 52 out of  100\n",
      "Doing 53 out of  100\n",
      "Doing 54 out of  100\n",
      "Doing 55 out of  100\n",
      "Doing 56 out of  100\n",
      "Doing 57 out of  100\n",
      "Doing 58 out of  100\n",
      "Doing 59 out of  100\n",
      "Doing 60 out of  100\n",
      "Doing 61 out of  100\n",
      "Doing 62 out of  100\n",
      "Doing 63 out of  100\n",
      "Doing 64 out of  100\n",
      "Doing 65 out of  100\n",
      "Doing 66 out of  100\n",
      "timed out\n",
      "Doing 67 out of  100\n",
      "Doing 68 out of  100\n",
      "Doing 69 out of  100\n",
      "Doing 70 out of  100\n",
      "Doing 71 out of  100\n",
      "Doing 72 out of  100\n",
      "Doing 73 out of  100\n",
      "Doing 74 out of  100\n",
      "Doing 75 out of  100\n",
      "Doing 76 out of  100\n",
      "Doing 77 out of  100\n",
      "Doing 78 out of  100\n",
      "Doing 79 out of  100\n",
      "Doing 80 out of  100\n",
      "Doing 81 out of  100\n",
      "Doing 82 out of  100\n",
      "Doing 83 out of  100\n",
      "Doing 84 out of  100\n",
      "Doing 85 out of  100\n",
      "Doing 86 out of  100\n",
      "Doing 87 out of  100\n",
      "Doing 88 out of  100\n",
      "Doing 89 out of  100\n",
      "Doing 90 out of  100\n",
      "Doing 91 out of  100\n",
      "Doing 92 out of  100\n",
      "Doing 93 out of  100\n",
      "Doing 94 out of  100\n",
      "Doing 95 out of  100\n",
      "Doing 96 out of  100\n",
      "Doing 97 out of  100\n",
      "Doing 98 out of  100\n",
      "Doing 99 out of  100\n",
      "Doing 100 out of  100\n"
     ]
    }
   ],
   "source": [
    "#get stations data\n",
    "import pandas as pd\n",
    "import acis\n",
    "import numpy as np\n",
    "counter = 1\n",
    "for k,s in sampleplaces.iteritems():\n",
    "    print \"Doing\", counter, \"out of \", len(sampleplaces.keys())\n",
    "    counter +=1\n",
    "    toprocess = ('urbanstations', 'ruralstations',)\n",
    "    for processkey in toprocess:\n",
    "        success = 0\n",
    "        for station in s['allstations'][processkey]:\n",
    "            try:\n",
    "                request = acis.StnDataRequest()  # change Request type\n",
    "                request.location(sid=station['sids'][0])  # change keyword and SID list\n",
    "                request.dates(\"2004-01-01\", \"2013-12-31\")  # sdate and edate\n",
    "                request.add_element(\"maxt\")\n",
    "                request.add_element(\"avgt\")\n",
    "                request.add_element(\"mint\")\n",
    "                request.metadata(\"name\")\n",
    "                result = acis.StnDataResult(request.submit())  # change Result type\n",
    "                \n",
    "                df = pd.DataFrame([x for x in result], columns=['uid', 'date', 'tmax', 'tavg','tmin'])\n",
    "                df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n",
    "                df = df.set_index(pd.DatetimeIndex(df['date']))\n",
    "                tempresults = []\n",
    "                for year in range(2004, 2014):\n",
    "                    subset = df[(df['date'] > '{0}-5-31'.format(year)) & (df['date'] <= '{0}-8-31'.format(year))]\n",
    "                    fails = 0\n",
    "                    for measures in ('tmin', 'tmax', 'tavg',):\n",
    "                        try:\n",
    "                            tempresults.append([measures, str(year), \\\n",
    "                                                subset[measures].astype(str).convert_objects(convert_numeric=True).mean()])\n",
    "                        except Exception,e:\n",
    "                            tempresults.append([measures, str(year), np.NaN])\n",
    "\n",
    "                station['results'] = tempresults\n",
    "                if processkey == 'urbanstations':\n",
    "                    if s['allstations'].get('urbanuse', False):\n",
    "                        s['allstations']['urbanuse'].append(station)\n",
    "                    else:\n",
    "                        s['allstations']['urbanuse'] = [station]\n",
    "                else:\n",
    "                    if s['allstations'].get('ruraluse', False):\n",
    "                        s['allstations']['ruraluse'].append(station)\n",
    "                    else:\n",
    "                        s['allstations']['ruraluse'] = [station]\n",
    "            except Exception, e:\n",
    "                if str(e).find(\"no data available\") == -1:\n",
    "                    print e\n",
    "                continue                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print sampleplaces.values()[0]\n",
    "for k,s in sampleplaces.iteritems():\n",
    "#     if s.get('usgsplacegeom'):\n",
    "#         del s['usgsplacegeom']\n",
    "#     if s.get('usgsplacegeomwgs84'):\n",
    "#         del s['usgsplacegeomwgs84']\n",
    "#     if s.get('earthenv'):\n",
    "#         del s['earthenv']\n",
    "    s['usgsplacegeomstr'] = str(s['usgsplacegeomstr'])\n",
    "    s['usgsplacegeomwgs84str'] = str(s['usgsplacegeomwgs84str'])\n",
    "    \n",
    "import pickle\n",
    "with open('climatecentral/climatecentralbase_allstations.pickle', 'wb') as fout:\n",
    "    pickle.dump(sampleplaces, fout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 , 22\n",
      "30 , 7\n",
      "28 , 4\n",
      "72 , 22\n",
      "6 , 3\n",
      "9 , 6\n",
      "3 , 2\n",
      "61 , 9\n",
      "4 , 5\n",
      "11 , 3\n",
      "7 , 6\n",
      "23 , 14\n",
      "16 , 11\n",
      "47 , 15\n",
      "4 , 4\n",
      "12 , 8\n",
      "6 , 3\n",
      "51 , 15\n",
      "6 , 5\n",
      "24 , 10\n",
      "35 , 34\n",
      "48 , 22\n",
      "10 , 2\n",
      "15 , 3\n",
      "6 , 4\n",
      "10 , 4\n",
      "6 , 4\n",
      "13 , 7\n",
      "25 , 14\n",
      "3 , 3\n",
      "45 , 16\n",
      "4 , 3\n",
      "13 , 4\n",
      "7 , 4\n",
      "0 , 3\n",
      "213 , 71\n",
      "185 , 58\n",
      "5 , 1\n",
      "7 , 1\n",
      "6 , 4\n",
      "98 , 29\n",
      "32 , 14\n",
      "7 , 2\n",
      "25 , 10\n",
      "14 , 2\n",
      "12 , 2\n",
      "6 , 7\n",
      "56 , 34\n",
      "10 , 4\n",
      "18 , 7\n",
      "26 , 9\n",
      "3 , 2\n",
      "29 , 16\n",
      "46 , 22\n",
      "46 , 16\n",
      "70 , 11\n",
      "25 , 9\n",
      "24 , 8\n",
      "17 , 7\n",
      "6 , 2\n",
      "188 , 65\n",
      "14 , 6\n",
      "23 , 5\n",
      "11 , 2\n",
      "15 , 5\n",
      "23 , 9\n",
      "1 , 7\n",
      "2 , 2\n",
      "2 , 3\n",
      "61 , 20\n",
      "18 , 9\n",
      "3 , 1\n",
      "128 , 31\n",
      "78 , 25\n",
      "24 , 13\n",
      "85 , 10\n",
      "21 , 8\n",
      "5 , 3\n",
      "0 , 3\n",
      "16 , 7\n",
      "18 , 12\n",
      "0 , 4\n",
      "12 , 4\n",
      "3 , 4\n",
      "17 , 7\n",
      "11 , 5\n",
      "9 , 3\n",
      "21 , 5\n",
      "6 , 5\n",
      "52 , 21\n",
      "241 , 86\n",
      "15 , 1\n",
      "4 , 1\n",
      "116 , 48\n",
      "11 , 2\n",
      "13 , 3\n",
      "538 , 112\n",
      "2 , 1\n",
      "26 , 8\n",
      "121 , 41\n"
     ]
    }
   ],
   "source": [
    "for s in sampleplaces.values():\n",
    "    print len(s['allstations'].get('ruraluse', [])), \",\", len(s['allstations'].get('urbanuse', []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "outputdict = []\n",
    "\n",
    "for s in sampleplaces.values():\n",
    "    urbanuse = s['allstations'].get('urbanuse', None)\n",
    "    if not urbanuse:\n",
    "        print \"skipping\", s['usgsplacename']\n",
    "        continue\n",
    "    urbandfs = []\n",
    "    for u in urbanuse:\n",
    "        tdf = pd.DataFrame(u['results'], columns=['measure', 'year', 'value'])\n",
    "        tdf.set_index(['measure', 'year'])\n",
    "        urbandfs.append(tdf)\n",
    "        \n",
    "    udf_concat = pd.concat(urbandfs)\n",
    "    urbanresult = pd.DataFrame(udf_concat.groupby(['measure','year'])['value'].mean()).reset_index()\n",
    "    \n",
    "    ruraluse = s['allstations'].get('ruraluse', [])\n",
    "    \n",
    "    if len(ruraluse) == 0:\n",
    "        outputdict.append({\n",
    "            'placename': s['usgsplacename'],\n",
    "            'uhitmin': \"no rural\",\n",
    "            'uhitavg': \"no rural\",\n",
    "            'uhitmax': \"no rural\"\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    ruraldfs = []\n",
    "    for r in ruraluse:\n",
    "        tdf = pd.DataFrame(r['results'], columns=['measure', 'year', 'value'])\n",
    "        tdf.set_index(['measure', 'year'])\n",
    "        ruraldfs.append(tdf)\n",
    "    df_concat = pd.concat(ruraldfs)\n",
    "    ruralresult = pd.DataFrame(df_concat.groupby(['measure','year'])['value'].mean()).reset_index()\n",
    "\n",
    "    \n",
    "    mergeddf =  urbanresult.merge(ruralresult,on=['measure','year'],how='left')\n",
    "#     print mergeddf[['value_x', 'value_y']].sub(axis=1)\n",
    "    mergeddf['uhi'] = mergeddf['value_x'] - mergeddf['value_y']\n",
    "    outputdict.append({\n",
    "            'placeid': s['placeid'],\n",
    "            'placename': s['usgsplacename'],\n",
    "            'uhitmin': mergeddf[mergeddf['measure']=='tmin']['uhi'].mean(),\n",
    "            'uhitavg': mergeddf[mergeddf['measure']=='tavg']['uhi'].mean(),\n",
    "            'uhitmax': mergeddf[mergeddf['measure']=='tmax']['uhi'].mean()\n",
    "        })\n",
    "#     print mergeddf[['value_x']].sub(mergeddf['value_y'], axis=0)\n",
    "#     for year in range(2004, 2014):\n",
    "#         for measure in ['tmin', 'tmax', 'tavg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('climatecentral/allstations.csv', 'w') as fin:\n",
    "    writer = csv.DictWriter(fin, fieldnames=['placeid', 'placename', 'uhitmin', 'uhitmax', 'uhitavg'])\n",
    "    writer.writeheader()\n",
    "    for row in outputdict:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
